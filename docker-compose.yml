services:
  # Service 1: Train both models (TF + RF)
  model-training:
    image: python:3.10-slim
    container_name: ml_trainer
    working_dir: /app
    volumes:
      - ./src:/app/src
      - ./requirements.txt:/app/requirements.txt
      - model_exchange:/exchange   # Shared volume mount
    # Run both training scripts -> Copy all artifacts to shared folder
    command: >
      sh -c "pip install -r requirements.txt &&
             python src/model_training.py &&
             python src/train_rf.py &&
             cp my_model.keras /exchange/my_model.keras &&
             cp tf_scaler.joblib /exchange/tf_scaler.joblib &&
             cp rf_model.joblib /exchange/rf_model.joblib &&
             cp scaler.joblib /exchange/scaler.joblib"

  # Service 2: Serve the model
  serving:
    image: python:3.10-slim
    container_name: ml_serving
    working_dir: /app
    ports:
      - "80:4000"
    volumes:
      - ./src:/app/src
      - ./requirements.txt:/app/requirements.txt
      - model_exchange:/exchange   # Shared volume mount
    depends_on:
      model-training:
        condition: service_completed_successfully
    # Copy all artifacts FROM shared folder -> Run server
    command: >
      sh -c "pip install --trusted-host pypi.python.org -r requirements.txt &&
             cp /exchange/my_model.keras ./my_model.keras &&
             cp /exchange/tf_scaler.joblib ./tf_scaler.joblib &&
             cp /exchange/rf_model.joblib ./rf_model.joblib &&
             cp /exchange/scaler.joblib ./scaler.joblib &&
             python src/main.py"

volumes:
  model_exchange: